{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mpd.slice.728000-728999.json\n",
      "mpd.slice.955000-955999.json\n",
      "mpd.slice.877000-877999.json\n",
      "mpd.slice.989000-989999.json\n",
      "mpd.slice.564000-564999.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from pandas.io.json import json_normalize\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "path_dir = '/root/team2/data_test'\n",
    "file_list = os.listdir(path_dir)\n",
    "\n",
    "import json\n",
    "head = []\n",
    "count = 0\n",
    "for f in file_list:\n",
    "    print f\n",
    "    with open(\"/root/team2/data_test/\"+f, 'rb') as infile:\n",
    "        file_data = json.load(infile)\n",
    "        playlist = file_data['playlists']\n",
    "        df = pd.DataFrame(playlist)\n",
    "        head += [df]\n",
    "        count += 1\n",
    "        if(count == 5):\n",
    "            break;\n",
    "df = pd.concat(head, ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data=1000 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0th try....\n",
      "precision : 0.00476647069407\n",
      "recall : 0.0279658995303\n",
      "f1score : 0.00814475943116\n",
      "\n",
      "1th try....\n",
      "precision : 0.00413538165328\n",
      "recall : 0.0252837830724\n",
      "f1score : 0.00710816188142\n",
      "\n",
      "2th try....\n",
      "precision : 0.00482826184582\n",
      "recall : 0.0347528433162\n",
      "f1score : 0.0084785822291\n",
      "\n",
      "3th try....\n",
      "precision : 0.00463671926863\n",
      "recall : 0.0328379319607\n",
      "f1score : 0.00812604076993\n",
      "\n",
      "4th try....\n",
      "precision : 0.00507647561656\n",
      "recall : 0.0348671377496\n",
      "f1score : 0.00886260203765\n",
      "\n",
      "5th try....\n",
      "precision : 0.00404402124106\n",
      "recall : 0.0381918739081\n",
      "f1score : 0.00731362499953\n",
      "\n",
      "6th try....\n",
      "precision : 0.00310526065121\n",
      "recall : 0.0198703294358\n",
      "f1score : 0.0053711397087\n",
      "\n",
      "7th try....\n",
      "precision : 0.00430621461783\n",
      "recall : 0.0357443559589\n",
      "f1score : 0.0076864257322\n",
      "\n",
      "8th try....\n",
      "precision : 0.00400197206625\n",
      "recall : 0.031518477208\n",
      "f1score : 0.00710216610063\n",
      "\n",
      "9th try....\n",
      "precision : 0.00437066431415\n",
      "recall : 0.0334488388921\n",
      "f1score : 0.00773112463685\n",
      "minF1score : 0.0053711397087, maxF1score: 0.00886260203765, averageF1score: 0.00759246275272\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "k = 10\n",
    "kfold = [df[i:i + len(df)/k] for i in xrange(0, len(df), len(df)/k)]\n",
    "\n",
    "max_f1score = 0\n",
    "min_f1score = 1\n",
    "average_f1score = 0\n",
    "for idx in range(0, k):\n",
    "    print \"\\n{}th try....\".format(idx)\n",
    "    test_df = kfold[idx].reset_index()\n",
    "    \n",
    "    train_df = []\n",
    "    for tidx in range(0, k):\n",
    "        if(tidx == idx):\n",
    "            continue\n",
    "        train_df += [kfold[tidx]]\n",
    "    train_df = pd.concat(train_df, ignore_index = True)\n",
    "    \n",
    "    train_playlist_name = list(set(train_df['name']))\n",
    "    test_playlist_name = list(set(test_df['name']))\n",
    "    \n",
    "    precision = 0\n",
    "    recall = 0\n",
    "    f1score = 0\n",
    "    count = 0\n",
    "    for name in test_playlist_name:\n",
    "        count += 1\n",
    "        #print count\n",
    "        test_playlist = []\n",
    "        for idx in range(0, len(test_df['name'])):\n",
    "            if(test_df['name'][idx] == name):\n",
    "                test = pd.DataFrame(test_df['tracks'][idx])\n",
    "                test_playlist += list(test['track_uri'])\n",
    "\n",
    "        random_train_playlist_name = random.sample(train_playlist_name, 10)\n",
    "        train_playlist = []\n",
    "        for train_name in random_train_playlist_name:\n",
    "            for idx in range(0, len(train_df['name'])):\n",
    "                if(train_df['name'][idx] == train_name):\n",
    "                    train = pd.DataFrame(train_df['tracks'][idx])\n",
    "                    temp =[]\n",
    "                    if(len(train_playlist) >= 500):\n",
    "                        continue\n",
    "                    elif(len(train_playlist) + len(list(train['track_uri'])) >= 500):\n",
    "                        temp = list(train['track_uri'])\n",
    "                        train_playlist += temp[:-(len(train_playlist)-500)]\n",
    "                    else:\n",
    "                        train_playlist += list(train['track_uri'])\n",
    "\n",
    "        precision += 1.0 *len(set(train_playlist).intersection(set(test_playlist)))/len(set(train_playlist))\n",
    "        recall += 1.0 * len(set(train_playlist).\n",
    "                                intersection(set(test_playlist)))/len(set(test_playlist))\n",
    "\n",
    "    precision = 1.0 * precision/ (count)\n",
    "    recall = 1.0 * recall / count\n",
    "    f1score = (2 * precision * recall) / (precision + recall)\n",
    "    print \"precision : {}\\nrecall : {}\\nf1score : {}\".format(precision, recall, f1score)\n",
    "    \n",
    "    if(min_f1score > f1score):\n",
    "        min_f1score = f1score\n",
    "    if(max_f1score < f1score):\n",
    "        max_f1score = f1score\n",
    "    \n",
    "    average_f1score += f1score\n",
    "\n",
    "average_f1score = average_f1score/10\n",
    "print \"minF1score : {}, maxF1score: {}, averageF1score: {}\".format(min_f1score, max_f1score, average_f1score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data=3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0th try....\n",
      "precision : 0.00370918925541\n",
      "recall : 0.0265073859279\n",
      "f1score : 0.00650774685591\n",
      "\n",
      "1th try....\n",
      "precision : 0.00513752845373\n",
      "recall : 0.0332354480119\n",
      "f1score : 0.00889939095481\n",
      "\n",
      "2th try....\n",
      "precision : 0.00465248045191\n",
      "recall : 0.0338889362801\n",
      "f1score : 0.00818172381547\n",
      "\n",
      "3th try....\n",
      "precision : 0.00437251916664\n",
      "recall : 0.0266467276556\n",
      "f1score : 0.00751232472342\n",
      "\n",
      "4th try....\n",
      "precision : 0.00403946486221\n",
      "recall : 0.030186177099\n",
      "f1score : 0.00712541794564\n",
      "\n",
      "5th try....\n",
      "precision : 0.00550336955326\n",
      "recall : 0.0359890374994\n",
      "f1score : 0.00954685386046\n",
      "\n",
      "6th try....\n",
      "precision : 0.0041011497872\n",
      "recall : 0.0281061106913\n",
      "f1score : 0.00715785001072\n",
      "\n",
      "7th try....\n",
      "precision : 0.004045407386\n",
      "recall : 0.0309653683024\n",
      "f1score : 0.00715594140247\n",
      "\n",
      "8th try....\n",
      "precision : 0.00479210287114\n",
      "recall : 0.0288227208315\n",
      "f1score : 0.00821788889762\n",
      "\n",
      "9th try....\n",
      "precision : 0.00382304538116\n",
      "recall : 0.0318296276987\n",
      "f1score : 0.0068261984668\n",
      "minF1score : 0.00650774685591, maxF1score: 0.00954685386046, averageF1score: 0.00771313369333\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "k = 10\n",
    "kfold = [df[i:i + len(df)/k] for i in xrange(0, len(df), len(df)/k)]\n",
    "\n",
    "max_f1score = 0\n",
    "min_f1score = 1\n",
    "average_f1score = 0\n",
    "for idx in range(0, k):\n",
    "    print \"\\n{}th try....\".format(idx)\n",
    "    test_df = kfold[idx].reset_index()\n",
    "    \n",
    "    train_df = []\n",
    "    for tidx in range(0, k):\n",
    "        if(tidx == idx):\n",
    "            continue\n",
    "        train_df += [kfold[tidx]]\n",
    "    train_df = pd.concat(train_df, ignore_index = True)\n",
    "    \n",
    "    train_playlist_name = list(set(train_df['name']))\n",
    "    test_playlist_name = list(set(test_df['name']))\n",
    "    \n",
    "    precision = 0\n",
    "    recall = 0\n",
    "    f1score = 0\n",
    "    count = 0\n",
    "    for name in test_playlist_name:\n",
    "        count += 1\n",
    "        #print count\n",
    "        test_playlist = []\n",
    "        for idx in range(0, len(test_df['name'])):\n",
    "            if(test_df['name'][idx] == name):\n",
    "                test = pd.DataFrame(test_df['tracks'][idx])\n",
    "                test_playlist += list(test['track_uri'])\n",
    "\n",
    "        random_train_playlist_name = random.sample(train_playlist_name, 10)\n",
    "        train_playlist = []\n",
    "        for train_name in random_train_playlist_name:\n",
    "            for idx in range(0, len(train_df['name'])):\n",
    "                if(train_df['name'][idx] == train_name):\n",
    "                    train = pd.DataFrame(train_df['tracks'][idx])\n",
    "                    temp =[]\n",
    "                    if(len(train_playlist) >= 500):\n",
    "                        continue\n",
    "                    elif(len(train_playlist) + len(list(train['track_uri'])) >= 500):\n",
    "                        temp = list(train['track_uri'])\n",
    "                        train_playlist += temp[:-(len(train_playlist)-500)]\n",
    "                    else:\n",
    "                        train_playlist += list(train['track_uri'])\n",
    "\n",
    "        precision += 1.0 *len(set(train_playlist).intersection(set(test_playlist)))/len(set(train_playlist))\n",
    "        recall += 1.0 * len(set(train_playlist).\n",
    "                                intersection(set(test_playlist)))/len(set(test_playlist))\n",
    "\n",
    "    precision = 1.0 * precision/ (count)\n",
    "    recall = 1.0 * recall / count\n",
    "    f1score = (2 * precision * recall) / (precision + recall)\n",
    "    print \"precision : {}\\nrecall : {}\\nf1score : {}\".format(precision, recall, f1score)\n",
    "    \n",
    "    if(min_f1score > f1score):\n",
    "        min_f1score = f1score\n",
    "    if(max_f1score < f1score):\n",
    "        max_f1score = f1score\n",
    "    \n",
    "    average_f1score += f1score\n",
    "\n",
    "average_f1score = average_f1score/10\n",
    "print \"minF1score : {}, maxF1score: {}, averageF1score: {}\".format(min_f1score, max_f1score, average_f1score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data=5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0th try....\n",
      "precision : 0.00484062634223\n",
      "recall : 0.0307875117316\n",
      "f1score : 0.00836590674431\n",
      "\n",
      "1th try....\n",
      "precision : 0.00433016374185\n",
      "recall : 0.0350033943032\n",
      "f1score : 0.0077069269289\n",
      "\n",
      "2th try....\n",
      "precision : 0.00493198304441\n",
      "recall : 0.0351086238643\n",
      "f1score : 0.0086489766754\n",
      "\n",
      "3th try....\n",
      "precision : 0.0047003071465\n",
      "recall : 0.0299989963005\n",
      "f1score : 0.00812722347089\n",
      "\n",
      "4th try....\n",
      "precision : 0.00429302319377\n",
      "recall : 0.0275861504237\n",
      "f1score : 0.0074298026051\n",
      "\n",
      "5th try....\n",
      "precision : 0.00451447332208\n",
      "recall : 0.0313129309919\n",
      "f1score : 0.00789124382891\n",
      "\n",
      "6th try....\n",
      "precision : 0.00462272239428\n",
      "recall : 0.0315649777841\n",
      "f1score : 0.00806440469874\n",
      "\n",
      "7th try....\n",
      "precision : 0.00438261627223\n",
      "recall : 0.0309982405666\n",
      "f1score : 0.00767948578163\n",
      "\n",
      "8th try....\n",
      "precision : 0.00428045917287\n",
      "recall : 0.0326027882679\n",
      "f1score : 0.00756738702722\n",
      "\n",
      "9th try....\n",
      "precision : 0.00500280881344\n",
      "recall : 0.0367862376301\n",
      "f1score : 0.00880778718308\n",
      "minF1score : 0.0074298026051, maxF1score: 0.00880778718308, averageF1score: 0.00802891449442\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "k = 10\n",
    "kfold = [df[i:i + len(df)/k] for i in xrange(0, len(df), len(df)/k)]\n",
    "\n",
    "max_f1score = 0\n",
    "min_f1score = 1\n",
    "average_f1score = 0\n",
    "for idx in range(0, k):\n",
    "    print \"\\n{}th try....\".format(idx)\n",
    "    test_df = kfold[idx].reset_index()\n",
    "    \n",
    "    train_df = []\n",
    "    for tidx in range(0, k):\n",
    "        if(tidx == idx):\n",
    "            continue\n",
    "        train_df += [kfold[tidx]]\n",
    "    train_df = pd.concat(train_df, ignore_index = True)\n",
    "    \n",
    "    train_playlist_name = list(set(train_df['name']))\n",
    "    test_playlist_name = list(set(test_df['name']))\n",
    "    \n",
    "    precision = 0\n",
    "    recall = 0\n",
    "    f1score = 0\n",
    "    count = 0\n",
    "    for name in test_playlist_name:\n",
    "        count += 1\n",
    "        #print count\n",
    "        test_playlist = []\n",
    "        for idx in range(0, len(test_df['name'])):\n",
    "            if(test_df['name'][idx] == name):\n",
    "                test = pd.DataFrame(test_df['tracks'][idx])\n",
    "                test_playlist += list(test['track_uri'])\n",
    "\n",
    "        random_train_playlist_name = random.sample(train_playlist_name, 10)\n",
    "        train_playlist = []\n",
    "        for train_name in random_train_playlist_name:\n",
    "            for idx in range(0, len(train_df['name'])):\n",
    "                if(train_df['name'][idx] == train_name):\n",
    "                    train = pd.DataFrame(train_df['tracks'][idx])\n",
    "                    temp =[]\n",
    "                    if(len(train_playlist) >= 500):\n",
    "                        continue\n",
    "                    elif(len(train_playlist) + len(list(train['track_uri'])) >= 500):\n",
    "                        temp = list(train['track_uri'])\n",
    "                        train_playlist += temp[:-(len(train_playlist)-500)]\n",
    "                    else:\n",
    "                        train_playlist += list(train['track_uri'])\n",
    "\n",
    "        precision += 1.0 *len(set(train_playlist).intersection(set(test_playlist)))/len(set(train_playlist))\n",
    "        recall += 1.0 * len(set(train_playlist).\n",
    "                                intersection(set(test_playlist)))/len(set(test_playlist))\n",
    "\n",
    "    precision = 1.0 * precision/ (count)\n",
    "    recall = 1.0 * recall / count\n",
    "    f1score = (2 * precision * recall) / (precision + recall)\n",
    "    print \"precision : {}\\nrecall : {}\\nf1score : {}\".format(precision, recall, f1score)\n",
    "    \n",
    "    if(min_f1score > f1score):\n",
    "        min_f1score = f1score\n",
    "    if(max_f1score < f1score):\n",
    "        max_f1score = f1score\n",
    "    \n",
    "    average_f1score += f1score\n",
    "\n",
    "average_f1score = average_f1score/10\n",
    "print \"minF1score : {}, maxF1score: {}, averageF1score: {}\".format(min_f1score, max_f1score, average_f1score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0th try....\n",
      "precision : 0.00472731437348\n",
      "recall : 0.0321534136682\n",
      "f1score : 0.00824274913545\n",
      "\n",
      "1th try....\n",
      "precision : 0.00440046633493\n",
      "recall : 0.0337465118559\n",
      "f1score : 0.00778569608321\n",
      "\n",
      "2th try....\n",
      "precision : 0.00507699963217\n",
      "recall : 0.0363818475904\n",
      "f1score : 0.00891055295593\n",
      "\n",
      "3th try....\n",
      "precision : 0.00490555190913\n",
      "recall : 0.0344464297395\n",
      "f1score : 0.00858806810189\n",
      "\n",
      "4th try....\n",
      "precision : 0.00420794629708\n",
      "recall : 0.0297721341235\n",
      "f1score : 0.00737370482887\n",
      "\n",
      "5th try....\n",
      "precision : 0.00447598542077\n",
      "recall : 0.02966153303\n",
      "f1score : 0.00777822146571\n",
      "\n",
      "6th try....\n",
      "precision : 0.00463316677478\n",
      "recall : 0.0338515897615\n",
      "f1score : 0.00815076279922\n",
      "\n",
      "7th try....\n",
      "precision : 0.00431932792689\n",
      "recall : 0.0304117464313\n",
      "f1score : 0.00756430994974\n",
      "\n",
      "8th try....\n",
      "precision : 0.00471793962386\n",
      "recall : 0.032851695217\n",
      "f1score : 0.00825093537543\n",
      "\n",
      "9th try....\n",
      "precision : 0.00458088833954\n",
      "recall : 0.0349830461162\n",
      "f1score : 0.00810098541714\n",
      "minF1score : 0.00737370482887, maxF1score: 0.00891055295593, averageF1score: 0.00807459861126\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "k = 10\n",
    "kfold = [df[i:i + len(df)/k] for i in xrange(0, len(df), len(df)/k)]\n",
    "\n",
    "max_f1score = 0\n",
    "min_f1score = 1\n",
    "average_f1score = 0\n",
    "for idx in range(0, k):\n",
    "    print \"\\n{}th try....\".format(idx)\n",
    "    test_df = kfold[idx].reset_index()\n",
    "    \n",
    "    train_df = []\n",
    "    for tidx in range(0, k):\n",
    "        if(tidx == idx):\n",
    "            continue\n",
    "        train_df += [kfold[tidx]]\n",
    "    train_df = pd.concat(train_df, ignore_index = True)\n",
    "    \n",
    "    train_playlist_name = list(set(train_df['name']))\n",
    "    test_playlist_name = list(set(test_df['name']))\n",
    "    \n",
    "    precision = 0\n",
    "    recall = 0\n",
    "    f1score = 0\n",
    "    count = 0\n",
    "    for name in test_playlist_name:\n",
    "        count += 1\n",
    "        #print count\n",
    "        test_playlist = []\n",
    "        for idx in range(0, len(test_df['name'])):\n",
    "            if(test_df['name'][idx] == name):\n",
    "                test = pd.DataFrame(test_df['tracks'][idx])\n",
    "                test_playlist += list(test['track_uri'])\n",
    "\n",
    "        random_train_playlist_name = random.sample(train_playlist_name, 10)\n",
    "        train_playlist = []\n",
    "        for train_name in random_train_playlist_name:\n",
    "            for idx in range(0, len(train_df['name'])):\n",
    "                if(train_df['name'][idx] == train_name):\n",
    "                    train = pd.DataFrame(train_df['tracks'][idx])\n",
    "                    temp =[]\n",
    "                    if(len(train_playlist) >= 500):\n",
    "                        continue\n",
    "                    elif(len(train_playlist) + len(list(train['track_uri'])) >= 500):\n",
    "                        temp = list(train['track_uri'])\n",
    "                        train_playlist += temp[:-(len(train_playlist)-500)]\n",
    "                    else:\n",
    "                        train_playlist += list(train['track_uri'])\n",
    "\n",
    "        precision += 1.0 *len(set(train_playlist).intersection(set(test_playlist)))/len(set(train_playlist))\n",
    "        recall += 1.0 * len(set(train_playlist).\n",
    "                                intersection(set(test_playlist)))/len(set(test_playlist))\n",
    "\n",
    "    precision = 1.0 * precision/ (count)\n",
    "    recall = 1.0 * recall / count\n",
    "    f1score = (2 * precision * recall) / (precision + recall)\n",
    "    print \"precision : {}\\nrecall : {}\\nf1score : {}\".format(precision, recall, f1score)\n",
    "    \n",
    "    if(min_f1score > f1score):\n",
    "        min_f1score = f1score\n",
    "    if(max_f1score < f1score):\n",
    "        max_f1score = f1score\n",
    "    \n",
    "    average_f1score += f1score\n",
    "\n",
    "average_f1score = average_f1score/10\n",
    "print \"minF1score : {}, maxF1score: {}, averageF1score: {}\".format(min_f1score, max_f1score, average_f1score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0th try....\n",
      "precision : 0.00511456937961\n",
      "recall : 0.0335054498425\n",
      "f1score : 0.0088744620674\n",
      "\n",
      "1th try....\n",
      "precision : 0.0046716239937\n",
      "recall : 0.0339092161456\n",
      "f1score : 0.0082119055563\n",
      "\n",
      "2th try....\n",
      "precision : 0.00441608198355\n",
      "recall : 0.0318256800807\n",
      "f1score : 0.00775595911531\n",
      "\n",
      "3th try....\n",
      "precision : 0.00517640632782\n",
      "recall : 0.0339219080584\n",
      "f1score : 0.00898215599735\n",
      "\n",
      "4th try....\n",
      "precision : 0.00458759953952\n",
      "recall : 0.0329837142471\n",
      "f1score : 0.0080548725632\n",
      "\n",
      "5th try....\n",
      "precision : 0.00473055542917\n",
      "recall : 0.0343279029645\n",
      "f1score : 0.00831523078069\n",
      "\n",
      "6th try....\n",
      "precision : 0.00526624736915\n",
      "recall : 0.0346195529821\n",
      "f1score : 0.00914185641043\n",
      "\n",
      "7th try....\n",
      "precision : 0.00485785450838\n",
      "recall : 0.0330733783433\n",
      "f1score : 0.0084714177744\n",
      "\n",
      "8th try....\n",
      "precision : 0.0044338016253\n",
      "recall : 0.0315789941618\n",
      "f1score : 0.00777584703324\n",
      "\n",
      "9th try....\n",
      "precision : 0.00440750362691\n",
      "recall : 0.0326269324068\n",
      "f1score : 0.00776592481586\n",
      "minF1score : 0.00775595911531, maxF1score: 0.00914185641043, averageF1score: 0.00833496321142\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "k = 10\n",
    "kfold = [df[i:i + len(df)/k] for i in xrange(0, len(df), len(df)/k)]\n",
    "\n",
    "max_f1score = 0\n",
    "min_f1score = 1\n",
    "average_f1score = 0\n",
    "for idx in range(0, k):\n",
    "    print \"\\n{}th try....\".format(idx)\n",
    "    test_df = kfold[idx].reset_index()\n",
    "    \n",
    "    train_df = []\n",
    "    for tidx in range(0, k):\n",
    "        if(tidx == idx):\n",
    "            continue\n",
    "        train_df += [kfold[tidx]]\n",
    "    train_df = pd.concat(train_df, ignore_index = True)\n",
    "    \n",
    "    train_playlist_name = list(set(train_df['name']))\n",
    "    test_playlist_name = list(set(test_df['name']))\n",
    "    \n",
    "    precision = 0\n",
    "    recall = 0\n",
    "    f1score = 0\n",
    "    count = 0\n",
    "    for name in test_playlist_name:\n",
    "        count += 1\n",
    "        #print count\n",
    "        test_playlist = []\n",
    "        for idx in range(0, len(test_df['name'])):\n",
    "            if(test_df['name'][idx] == name):\n",
    "                test = pd.DataFrame(test_df['tracks'][idx])\n",
    "                test_playlist += list(test['track_uri'])\n",
    "\n",
    "        random_train_playlist_name = random.sample(train_playlist_name, 10)\n",
    "        train_playlist = []\n",
    "        for train_name in random_train_playlist_name:\n",
    "            for idx in range(0, len(train_df['name'])):\n",
    "                if(train_df['name'][idx] == train_name):\n",
    "                    train = pd.DataFrame(train_df['tracks'][idx])\n",
    "                    temp =[]\n",
    "                    if(len(train_playlist) >= 500):\n",
    "                        continue\n",
    "                    elif(len(train_playlist) + len(list(train['track_uri'])) >= 500):\n",
    "                        temp = list(train['track_uri'])\n",
    "                        train_playlist += temp[:-(len(train_playlist)-500)]\n",
    "                    else:\n",
    "                        train_playlist += list(train['track_uri'])\n",
    "\n",
    "        precision += 1.0 *len(set(train_playlist).intersection(set(test_playlist)))/len(set(train_playlist))\n",
    "        recall += 1.0 * len(set(train_playlist).\n",
    "                                intersection(set(test_playlist)))/len(set(test_playlist))\n",
    "\n",
    "    precision = 1.0 * precision/ (count)\n",
    "    recall = 1.0 * recall / count\n",
    "    f1score = (2 * precision * recall) / (precision + recall)\n",
    "    print \"precision : {}\\nrecall : {}\\nf1score : {}\".format(precision, recall, f1score)\n",
    "    \n",
    "    if(min_f1score > f1score):\n",
    "        min_f1score = f1score\n",
    "    if(max_f1score < f1score):\n",
    "        max_f1score = f1score\n",
    "    \n",
    "    average_f1score += f1score\n",
    "\n",
    "average_f1score = average_f1score/10\n",
    "print \"minF1score : {}, maxF1score: {}, averageF1score: {}\".format(min_f1score, max_f1score, average_f1score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
