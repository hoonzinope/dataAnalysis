{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from pandas.io.json import json_normalize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim import corpora, models, similarities\n",
    "from gensim.corpora import Dictionary\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_category_1(file_path):\n",
    "\n",
    "\n",
    "    f = open(file_path).read()\n",
    "    data = json.loads(f)\n",
    "    playlists = data[\"playlists\"]\n",
    "    df = pd.DataFrame(playlists)\n",
    "    train, test = train_test_split(df, test_size=0.1)\n",
    "\n",
    "    r_test_data = {'pid' : list(test['pid']), 'name' : list(test['name']), 'num_holdouts' : list(test['num_tracks']), 'tracks' : [{} for _ in range(test.shape[0])], 'num_samples' : [0 for _ in range(test.shape[0])], 'num_tracks' : test['num_tracks']}\n",
    "    r_test = pd.DataFrame(r_test_data)\n",
    "    \n",
    "    train = train.T.to_dict().values()\n",
    "    test = test.T.to_dict().values()\n",
    "    r_test = r_test.T.to_dict().values()\n",
    "    \n",
    "    with open('./train.json', 'w+') as f:\n",
    "        f.write(json.dumps(train))\n",
    "    with open('./test_R.json', 'w+') as f:\n",
    "        f.write(json.dumps(test))\n",
    "    with open('./test.json', 'w+') as f:\n",
    "        f.write(json.dumps(r_test))\n",
    "        \n",
    "        \n",
    "make_category_1('mpd.slice.0-999.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> train_set </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('./train.json').read()\n",
    "data = json.loads(f)\n",
    "train_df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> test_set </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('./test_R.json').read()\n",
    "data = json.loads(f)\n",
    "test_df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> cross validation test </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision : 0.694158157114, recall : 9.73639958135, f1score : 1.29592325929, topic_num : 3\n",
      "precision : 0.574347039305, recall : 3.59026176993, f1score : 0.990276067858, topic_num : 5\n",
      "precision : 0.372478138421, recall : 3.52351469063, f1score : 0.673734398523, topic_num : 7\n",
      "precision : 0.419944777889, recall : 7.15032759897, f1score : 0.793298466925, topic_num : 9\n",
      "precision : 0.469922455055, recall : 6.14524488033, f1score : 0.873081031717, topic_num : 11\n",
      "precision : 0.508062110726, recall : 6.5068008261, f1score : 0.942529880213, topic_num : 13\n",
      "precision : 0.416021284851, recall : 5.37815959082, f1score : 0.772302042728, topic_num : 15\n",
      "precision : 0.673415969457, recall : 8.53631585984, f1score : 1.24835153225, topic_num : 17\n",
      "precision : 0.594075794616, recall : 6.60001373349, f1score : 1.09003603246, topic_num : 19\n",
      "precision : 0.489515144385, recall : 4.63085933351, f1score : 0.885433588913, topic_num : 21\n",
      "max_f1score : 1.29592325929, max_topic_num : 3\n"
     ]
    }
   ],
   "source": [
    "train_playlist_name = list(set(train_df['name']))\n",
    "\n",
    "train_playlist_name = [train_playlist_name[i:i + len(train_playlist_name)/10] \n",
    "                       for i in xrange(0, len(train_playlist_name), len(train_playlist_name)/10)]\n",
    "topic_num = 3\n",
    "max = 0\n",
    "max_topic_num = 0\n",
    "for idx in range(0, len(train_playlist_name)):\n",
    "    #0~9 까지 validation\n",
    "    validation_texts = [[word for word in name.lower().split()]for name in train_playlist_name[idx]]\n",
    "    \n",
    "    #train_set after slice\n",
    "    train_texts = []\n",
    "    for tidx in range(0,len(train_playlist_name)):\n",
    "        if(tidx == idx):\n",
    "            continue\n",
    "        train_texts.extend(train_playlist_name[tidx])\n",
    "    train_texts = [[word for word in name.lower().split()]\n",
    "                  for name in train_texts]\n",
    "    \n",
    "    #LDA model create\n",
    "    dictionary = corpora.Dictionary(train_texts)\n",
    "    corpus = [dictionary.doc2bow(text) for text in train_texts]\n",
    "    model = models.ldamodel.LdaModel(corpus = corpus, id2word = dictionary,\n",
    "                                    num_topics = topic_num, random_state = 0,\n",
    "                                    chunksize = 2000, passes = 10)\n",
    "    \n",
    "    #document 별 topic 다항분포 train\n",
    "    train_dic = {}\n",
    "    for idx in range(0, len(train_texts)):\n",
    "        t = train_texts[idx]\n",
    "        bow = dictionary.doc2bow(t)\n",
    "        get_document_topics = tuple(model.get_document_topics(bow))\n",
    "        name = ' '.join(t)\n",
    "        train_dic[name] = dict(get_document_topics)\n",
    "    train_dic = pd.DataFrame(train_dic)\n",
    "    #print train_dic\n",
    "    \n",
    "    #document 별 topic 다항분포 validation\n",
    "    validation_dic = {}\n",
    "    for idx in range(0, len(validation_texts)):\n",
    "        t = validation_texts[idx]\n",
    "        bow = dictionary.doc2bow(t)\n",
    "        get_document_topics = tuple(model.get_document_topics(bow))\n",
    "        name = ' '.join(t)\n",
    "        validation_dic[name] = dict(get_document_topics)\n",
    "    validation_dic =pd.DataFrame(validation_dic).T\n",
    "    #print validation_dic\n",
    "    \n",
    "    result = []\n",
    "    for trainName in train_dic.columns:\n",
    "        idx = 0\n",
    "        #c = 0\n",
    "        denominator = 0\n",
    "        numeratorA = 0\n",
    "        numeratorB = 0\n",
    "        columns_ones = []\n",
    "        for valName in validation_dic.index:\n",
    "            for idx in range(0, topic_num):\n",
    "                #c += pow(train_dic[trainName][idx] - validation_dic[idx][valName], 2)\n",
    "                denominator += (train_dic[trainName][idx] * validation_dic[idx][valName])\n",
    "                numeratorA += pow(train_dic[trainName][idx],2)\n",
    "                numeratorB += pow(validation_dic[idx][valName],2)\n",
    "            #c = 1/(1+sqrt(c))\n",
    "            c = denominator / (sqrt(numeratorA) * sqrt(numeratorB))\n",
    "            columns_ones.append(c)\n",
    "        result.append(columns_ones)\n",
    "    #train_playlist X validation_playlist_name 유클리디안 거리 행렬 생성    \n",
    "    result = pd.DataFrame(result, columns = validation_dic.index, index = train_dic.columns)\n",
    "    \n",
    "    #validation_Set evaluation\n",
    "    count = 0\n",
    "    precision = 0\n",
    "    recall = 0\n",
    "    f1score = 0\n",
    "    for name in result.columns:\n",
    "        #test_playlist_name 과 거리가 가까운 train_playlist_name 상위 10개 도출\n",
    "        result_playlist_name = result[name].sort_values(ascending = False).head(10).index\n",
    "        #print count\n",
    "        count+=1\n",
    "    \n",
    "        #test_playlist 생성\n",
    "        validation_playlist = []\n",
    "        for idx in range(0, len(train_df['name'])):\n",
    "            if(train_df['name'][idx].lower() == name):\n",
    "                val_track_uri = pd.DataFrame(train_df['tracks'][idx])\n",
    "                val_playlist = list(val_track_uri['track_uri'])\n",
    "        #print len(test_playlist)        \n",
    "        if(len(val_playlist) == 0):\n",
    "            continue\n",
    "            \n",
    "        #비교할 train_playlist 생성\n",
    "        train_playlist = []\n",
    "        for train_name in result_playlist_name:\n",
    "            for idx in range(0,len(train_df['name'])):\n",
    "                if(train_df['name'][idx].lower() == train_name):\n",
    "                    df = pd.DataFrame(train_df['tracks'][idx])\n",
    "                    train_playlist += list(df['track_uri'])\n",
    "                    if(len(train_playlist) >= 500):\n",
    "                        continue\n",
    "                    elif(len(train_playlist) + len(list(df['track_uri'])) >= 500):\n",
    "                        temp = list(df['track_uri'])\n",
    "                        train_playlist += temp[:-(len(train_playlist)-500)]\n",
    "                    else:\n",
    "                        train_playlist += list(df['track_uri'])\n",
    "        #print len(train_playlist)      \n",
    "        #r-precision 계산\n",
    "        precision +=1.0 *len(set(train_playlist).intersection(set(val_playlist)))/len(set(train_playlist)) *100\n",
    "        recall += 1.0 * len(set(train_playlist).intersection(set(val_playlist)))/len(set(val_playlist))*100\n",
    "    \n",
    "    #avarage result\n",
    "    precision = 1.0 * precision/ (count)\n",
    "    recall = 1.0 * recall / count\n",
    "    f1score = (2 * precision * recall) / (precision + recall)\n",
    "    print \"precision : {}, recall : {}, f1score : {}, topic_num : {}\".format(precision, recall, f1score, topic_num)\n",
    "    if max < f1score:\n",
    "        max = f1score\n",
    "        max_topic_num = topic_num\n",
    "        maxModel = model\n",
    "\n",
    "    topic_num += 2 #3,5,7,9,11,13,15,....\n",
    "print (\"max_f1score : {}, max_topic_num : {}\".format(max, max_topic_num)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> test! </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision : 0.426891615542\n",
      "recall : 3.18891278753\n",
      "f1score : 0.752983281139\n"
     ]
    }
   ],
   "source": [
    "train_playlist_name = list(set(train_df['name']))\n",
    "test_playlist_name = list(set(test_df['name']))\n",
    "\n",
    "#train_playlist_name\n",
    "#소문자로\n",
    "train_texts = [[word for word in name.lower().split()]\n",
    "         for name in train_playlist_name]\n",
    "\n",
    "#test_playlist_name\n",
    "#역시 소문자로\n",
    "test_texts = [[word for word in name.lower().split()]\n",
    "         for name in test_playlist_name]\n",
    "\n",
    "topic_num = max_topic_num\n",
    " #LDA model create\n",
    "#dictionary = corpora.Dictionary(train_texts)\n",
    "#corpus = [dictionary.doc2bow(text) for text in train_texts]\n",
    "#model = models.ldamodel.LdaModel(corpus = corpus, id2word = dictionary,\n",
    "#                                   num_topics = topic_num, random_state = 0,\n",
    "#                                    chunksize = 20000, passes = 10)\n",
    "model = maxModel   \n",
    "    #document 별 topic 다항분포 train\n",
    "train_dic = {}\n",
    "for idx in range(0, len(train_texts)):\n",
    "    t = train_texts[idx]\n",
    "    bow = dictionary.doc2bow(t)\n",
    "    get_document_topics = tuple(model.get_document_topics(bow))\n",
    "    name = ' '.join(t)\n",
    "    train_dic[name] = dict(get_document_topics)\n",
    "train_dic = pd.DataFrame(train_dic)\n",
    "#print train_dic\n",
    "    \n",
    "    #document 별 topic 다항분포 validation\n",
    "test_dic = {}\n",
    "for idx in range(0, len(test_texts)):\n",
    "    t = test_texts[idx]\n",
    "    bow = dictionary.doc2bow(t)\n",
    "    get_document_topics = tuple(model.get_document_topics(bow))\n",
    "    name = ' '.join(t)\n",
    "    test_dic[name] = dict(get_document_topics)\n",
    "test_dic =pd.DataFrame(test_dic).T\n",
    "#print validation_dic\n",
    "    \n",
    "result = []\n",
    "for trainName in train_dic.columns:\n",
    "    idx = 0\n",
    "    c = 0\n",
    "    columns_ones = []\n",
    "    for testName in test_dic.index:\n",
    "        for idx in range(0, topic_num):\n",
    "            #c += pow(train_dic[trainName][idx] - test_dic[idx][testName], 2)\n",
    "            denominator += (train_dic[trainName][idx] * test_dic[idx][testName])\n",
    "            numeratorA += pow(train_dic[trainName][idx],2)\n",
    "            numeratorB += pow(test_dic[idx][testName],2)\n",
    "        #c = 1/(1+sqrt(c))\n",
    "        c = 1.0 *denominator / (sqrt(numeratorA) * sqrt(numeratorB))\n",
    "        columns_ones.append(c)\n",
    "    result.append(columns_ones)\n",
    "#train_playlist X validation_playlist_name 코사인 유사도 행렬 생성    \n",
    "result = pd.DataFrame(result, columns = test_dic.index, index = train_dic.columns)\n",
    "#print result   \n",
    "    #validation_Set evaluation\n",
    "count = 0\n",
    "precision = 0\n",
    "recall = 0\n",
    "f1score = 0\n",
    "for name in result.columns:\n",
    "    #test_playlist_name 과 거리가 가까운 train_playlist_name 상위 10개 도출\n",
    "    result_playlist_name = result[name].sort_values(ascending = False).head(10).index\n",
    "    #print count\n",
    "    \n",
    "    #test_playlist 생성\n",
    "    test_playlist = []\n",
    "    for idx in range(0, len(test_df['name'])):\n",
    "        if(test_df['name'][idx].lower() == name):\n",
    "            test_track_uri = pd.DataFrame(test_df['tracks'][idx])\n",
    "            test_playlist = list(test_track_uri['track_uri'])\n",
    "    #print \"test_playlist length :{}\".format(len(test_playlist))        \n",
    "    if(len(test_playlist) == 0):\n",
    "        continue\n",
    "            \n",
    "        #비교할 train_playlist 생성\n",
    "    train_playlist = []\n",
    "    for train_name in result_playlist_name:\n",
    "        for idx in range(0,len(train_df['name'])):\n",
    "            if(train_df['name'][idx].lower() == train_name):\n",
    "                df = pd.DataFrame(train_df['tracks'][idx])\n",
    "                temp =[]\n",
    "                if(len(train_playlist) >= 500):\n",
    "                    continue\n",
    "                elif(len(train_playlist) + len(list(df['track_uri'])) >= 500):\n",
    "                    temp = list(df['track_uri'])\n",
    "                    train_playlist += temp[:-(len(train_playlist)-500)]\n",
    "                else:\n",
    "                    train_playlist += list(df['track_uri'])\n",
    "    #print \"total_train_playlist : {}\".format(len(train_playlist))      \n",
    "    #r-precision 계산\n",
    "    #if(len(set(train_playlist).intersection(set(test_playlist))) == 0):\n",
    "    #    continue\n",
    "    precision += 1.0 *len(set(train_playlist).intersection(set(test_playlist)))/len(set(train_playlist)) *100\n",
    "    recall += 1.0 * len(set(train_playlist).\n",
    "                            intersection(set(test_playlist)))/len(set(test_playlist))*100\n",
    "    count+=1\n",
    "    #avarage result\n",
    "precision = 1.0 * precision/ (count)\n",
    "recall = 1.0 * recall / count\n",
    "f1score = (2 * precision * recall) / (precision + recall)\n",
    "print \"precision : {}\\nrecall : {}\\nf1score : {}\".format(precision, recall, f1score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
